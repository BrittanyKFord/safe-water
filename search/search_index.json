{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Safe Water \u00b6 This is a Code for Boston project that is trying to predict health-based drinking water violations using the Environmental Protection Agency's Safe Drinking Water Information System . This project will analyze data from the EPA\u2019s Safe Drinking Water Information System and then integrate other data sets to try to predict health-based drinking water violations in the United States. The Conservation Law Foundation has expressed interest in this project. They might be a potential long-term partner. We will explore other datasets from the EPA, including : the Toxic Release Inventory database , the Superfund Enterprise Management System , the Environmental Radiation Monitoring database , the Enforcement and Compliance History Outline . Tip Find us on our Slack channel #water. Join our Trello board . We are using python with pandas , although some people are using R as well.","title":"Home"},{"location":"#safe-water","text":"This is a Code for Boston project that is trying to predict health-based drinking water violations using the Environmental Protection Agency's Safe Drinking Water Information System . This project will analyze data from the EPA\u2019s Safe Drinking Water Information System and then integrate other data sets to try to predict health-based drinking water violations in the United States. The Conservation Law Foundation has expressed interest in this project. They might be a potential long-term partner. We will explore other datasets from the EPA, including : the Toxic Release Inventory database , the Superfund Enterprise Management System , the Environmental Radiation Monitoring database , the Enforcement and Compliance History Outline . Tip Find us on our Slack channel #water. Join our Trello board . We are using python with pandas , although some people are using R as well.","title":"Safe Water"},{"location":"additional-links/","text":"Additional links \u00b6 Research links \u00b6 National trends in drinking water quality violations is a relevant paper that uses the same sources of information that we use (SWDIS, additional census information). The supplementary materials go into the data processing details. Here are the places that struggle to meet the rules on safe drinking water , a New York Times article about the PNAS articlee Threats on Tap , an NRDC article about safe drinking water violations. Zip of scraped CSVs \u00b6 swdis.zip is a zip of scraped CSVs. The quality is probably dodgy, so use at your own risk. SDWIS List of Action Codes \u00b6 SDWIS -- List Validations by Element gives a rundown of action codes found in the database.","title":"Additional Links"},{"location":"additional-links/#additional-links","text":"","title":"Additional links"},{"location":"additional-links/#research-links","text":"National trends in drinking water quality violations is a relevant paper that uses the same sources of information that we use (SWDIS, additional census information). The supplementary materials go into the data processing details. Here are the places that struggle to meet the rules on safe drinking water , a New York Times article about the PNAS articlee Threats on Tap , an NRDC article about safe drinking water violations.","title":"Research links"},{"location":"additional-links/#zip-of-scraped-csvs","text":"swdis.zip is a zip of scraped CSVs. The quality is probably dodgy, so use at your own risk.","title":"Zip of scraped CSVs"},{"location":"additional-links/#sdwis-list-of-action-codes","text":"SDWIS -- List Validations by Element gives a rundown of action codes found in the database.","title":"SDWIS List of Action Codes"},{"location":"getting-started/","text":"Getting started \u00b6 Installing python and dependencies \u00b6 The easiest way to install the Python dependencies is using Pipenv. Ensure that you have Pipenv installed , then, with the repo as your working directory, run: pipenv install To add a new Python dependency, run: pipenv install antigravity # Replace `antigravity` with desired package name Be sure to commit Pipfile and Pipfile.lock to the repo. Running the notebooks \u00b6 To run the notebooks, run: pipenv run jupyter notebook jupyter_notebooks Installing R and dependencies \u00b6 Install RStudio and install the following packages: install.packages ( c ( \"tidyverse\" , \"noncensus\" , \"ggplot2\" , \"choroplethr\" , \"choroplethrMaps\" , \"lubridate\" )) Make sure the symlink in the R directory points to the data directory. You should unpack a copy of the scraped CSVs (see additional links ).","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting started"},{"location":"getting-started/#installing-python-and-dependencies","text":"The easiest way to install the Python dependencies is using Pipenv. Ensure that you have Pipenv installed , then, with the repo as your working directory, run: pipenv install To add a new Python dependency, run: pipenv install antigravity # Replace `antigravity` with desired package name Be sure to commit Pipfile and Pipfile.lock to the repo.","title":"Installing python and dependencies"},{"location":"getting-started/#running-the-notebooks","text":"To run the notebooks, run: pipenv run jupyter notebook jupyter_notebooks","title":"Running the notebooks"},{"location":"getting-started/#installing-r-and-dependencies","text":"Install RStudio and install the following packages: install.packages ( c ( \"tidyverse\" , \"noncensus\" , \"ggplot2\" , \"choroplethr\" , \"choroplethrMaps\" , \"lubridate\" )) Make sure the symlink in the R directory points to the data directory. You should unpack a copy of the scraped CSVs (see additional links ).","title":"Installing R and dependencies"},{"location":"data/data-cleanup/","text":"Data Cleanup \u00b6","title":"Data Cleanup"},{"location":"data/data-cleanup/#data-cleanup","text":"","title":"Data Cleanup"},{"location":"data/data-format/","text":"Data Format \u00b6 SDWIS data schema \u00b6 The SDWIS Model gives very minimal information about the actual data schema. SDWIS List of Action Codes \u00b6 SDWIS -- List Validations by Element gives a rundown of action codes found in the database. Contaminant codes \u00b6 Contaminant codes can be found in various locations on the internet and have been compiled into 2 CSV files located in the data/ directory.","title":"Data Format"},{"location":"data/data-format/#data-format","text":"","title":"Data Format"},{"location":"data/data-format/#sdwis-data-schema","text":"The SDWIS Model gives very minimal information about the actual data schema.","title":"SDWIS data schema"},{"location":"data/data-format/#sdwis-list-of-action-codes","text":"SDWIS -- List Validations by Element gives a rundown of action codes found in the database.","title":"SDWIS List of Action Codes"},{"location":"data/data-format/#contaminant-codes","text":"Contaminant codes can be found in various locations on the internet and have been compiled into 2 CSV files located in the data/ directory.","title":"Contaminant codes"},{"location":"data/web-scraper/","text":"Running the scraper \u00b6 To run the scraper, run: pipenv run python -i swid-db-scraper.py This will load the file and put you into a command prompt. From that prompt, run the following: pull_envirofacts_data () Warning The scraper can take hours to run. Data aggregation \u00b6 The web-scraper functions as a command-line program with a flag based interface. The --help and -h flag is supported to get futher information however flags covered are listed here -p takes a number of process that the script can use to process all of the different data sets -l takes a pathname to a logfile which the script will write too -rs takes a number which maps to the number of records to be included in any one request -mq takes a number which maps to the number of times a url should be attemped before giving up Webscraping TODO \u00b6 test parallel implementation of web scraper adjust webscraper to reference file name update list of databases we want beyond SWDIS databases test that data read in how can we join and aggreate data so that we dont always have to navigate additional print out info to better understand were we are in the scripts additional error handlings Running scripts on your machine \u00b6 (instructions only tested on solus linux but should hold on other os') in the command line cd to the safe water directory run the command 'python3.6 -i swid-db-scraper.py' this will load the file and put you into a command prompt now run the following command: 'pull_envirofacts_data()'","title":"Web Scraper"},{"location":"data/web-scraper/#running-the-scraper","text":"To run the scraper, run: pipenv run python -i swid-db-scraper.py This will load the file and put you into a command prompt. From that prompt, run the following: pull_envirofacts_data () Warning The scraper can take hours to run.","title":"Running the scraper"},{"location":"data/web-scraper/#data-aggregation","text":"The web-scraper functions as a command-line program with a flag based interface. The --help and -h flag is supported to get futher information however flags covered are listed here -p takes a number of process that the script can use to process all of the different data sets -l takes a pathname to a logfile which the script will write too -rs takes a number which maps to the number of records to be included in any one request -mq takes a number which maps to the number of times a url should be attemped before giving up","title":"Data aggregation"},{"location":"data/web-scraper/#webscraping-todo","text":"test parallel implementation of web scraper adjust webscraper to reference file name update list of databases we want beyond SWDIS databases test that data read in how can we join and aggreate data so that we dont always have to navigate additional print out info to better understand were we are in the scripts additional error handlings","title":"Webscraping TODO"},{"location":"data/web-scraper/#running-scripts-on-your-machine","text":"(instructions only tested on solus linux but should hold on other os') in the command line cd to the safe water directory run the command 'python3.6 -i swid-db-scraper.py' this will load the file and put you into a command prompt now run the following command: 'pull_envirofacts_data()'","title":"Running scripts on your machine"}]}